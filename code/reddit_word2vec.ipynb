{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit and word2vec\n",
    "\n",
    "This is some of the code for my paper 'The Meanings of Class in Reddit Comments - An Exploratory Study of Word Embeddings'\n",
    "\n",
    "This notebook accesses the data, does preprocessing and runs the word2vec implementation and some evaluation tests.\n",
    "\n",
    "More details on request: jonas.schwenke@uni-konstanz.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import google.auth\n",
    "from google.cloud.bigquery.client import Client\n",
    "import scipy\n",
    "import cython\n",
    "from gensim import models, similarities\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import logging\n",
    "import sys  \n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Google BigQuery\n",
    "\n",
    "WARNING: Big requests might cost money if account it set up and test period is expired.\n",
    "\n",
    "To access GBQ you first need to create a google account and install the necessary packages. \n",
    "<br>See https://cloud.google.com/bigquery/docs/reference/libraries?hl=de for more information.\n",
    "<br>The query is written in Legacy SQL to make use of the RAND() function for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the path to GBG project ssh keys\n",
    "keys_path = 'ssh_keys.json'\n",
    "\n",
    "with open(keys_path, 'r') as f:\n",
    "        json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = keys_path\n",
    "bq_client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# all tables to be uses for sampling\n",
    "all_years = [ \"[fh-bigquery:reddit_comments.2008],\"\n",
    "            \"[fh-bigquery:reddit_comments.2009],\"\n",
    "            \"[fh-bigquery:reddit_comments.2010],\"\n",
    "            \"[fh-bigquery:reddit_comments.2011],\"\n",
    "            \"[fh-bigquery:reddit_comments.2012],\"\n",
    "            \"[fh-bigquery:reddit_comments.2013],\"\n",
    "            \"[fh-bigquery:reddit_comments.2014],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_01],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_02],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_03],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_04],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_05],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_06],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_07],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_08],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_09],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_10],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_11],\"\n",
    "            \"[fh-bigquery:reddit_comments.2015_12],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_01],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_02],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_03],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_04],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_05],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_06],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_07],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_08],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_09],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_10],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_11],\"\n",
    "            \"[fh-bigquery:reddit_comments.2016_12],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_01],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_02],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_03],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_04],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_05],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_06],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_07],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_08],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_09],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_10],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_11],\"\n",
    "            \"[fh-bigquery:reddit_comments.2017_12],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_01],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_02],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_03],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_04],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_05],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_06],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_07],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_08],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_09],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_10],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_11],\"\n",
    "            \"[fh-bigquery:reddit_comments.2018_12])\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10000000/10000000 [30:42<00:00, 5428.77rows/s]\n"
     ]
    }
   ],
   "source": [
    "# downloads query object as pandas data frame\n",
    "query = (\"SELECT body, author, created_utc, parent_id, score, subreddit FROM\"\n",
    "        \"[fh-bigquery:reddit_comments.2008],\"\n",
    "        \"[fh-bigquery:reddit_comments.2009],\"\n",
    "        \"[fh-bigquery:reddit_comments.2010],\"\n",
    "        \"[fh-bigquery:reddit_comments.2011],\"\n",
    "        \"[fh-bigquery:reddit_comments.2012],\"\n",
    "        \"[fh-bigquery:reddit_comments.2013],\"\n",
    "        \"[fh-bigquery:reddit_comments.2014],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_01],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_02],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_03],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_04],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_05],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_06],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_07],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_08],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_09],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_10],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_11],\"\n",
    "        \"[fh-bigquery:reddit_comments.2015_12],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_01],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_02],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_03],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_04],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_05],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_06],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_07],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_08],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_09],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_10],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_11],\"\n",
    "        \"[fh-bigquery:reddit_comments.2016_12],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_01],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_02],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_03],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_04],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_05],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_06],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_07],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_08],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_09],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_10],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_11],\"\n",
    "        \"[fh-bigquery:reddit_comments.2017_12],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_01],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_02],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_03],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_04],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_05],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_06],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_07],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_08],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_09],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_10],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_11],\"\n",
    "        \"[fh-bigquery:reddit_comments.2018_12]\"\n",
    "        \"WHERE subreddit='AskReddit'\"\n",
    "        \"LIMIT 10\")\n",
    "credentials, project = google.auth.default()\n",
    "\n",
    "worldnews = pd.read_gbq(query, \n",
    "                        location=\"US\", \n",
    "                        credentials=credentials, \n",
    "                        dialect='legacy', \n",
    "                        project_id='google_big_query_projectname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# save compressed pandas data frame\n",
    "os.chdir('filepath')\n",
    "worldnews.to_csv('worldnews.gz', \n",
    "               sep='|', \n",
    "               compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This part loads two botlists, conducts preprocessing on chunked, compressed files and saves process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# read botlists \n",
    "# 1. https://www.reddit.com/r/autowikibot/wiki/redditbots\n",
    "# 2. Custom botlist from top 50 authors\n",
    "\n",
    "botlist = pd.read_csv('redditbots.csv')\n",
    "botlist['bots'] = botlist['bots'].map(lambda x: x.lstrip('/u/'))\n",
    "botlist = botlist['bots'].values.tolist()\n",
    "\n",
    "top50bots = pd.read_csv('top50bots.csv',header=None).iloc[:][0].values.tolist()\n",
    "botlist = botlist + top50bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocessing - drop NA, bots & duplicates with optional print statements\n",
    "def drop(data, botlist):\n",
    "    \n",
    "    # stop time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # print('-- Rows before preprocessing: ' + str(len(data)))\n",
    "    \n",
    "    # drop NA\n",
    "    data = data.dropna(subset=['body'])\n",
    "    # print('-- Rows after dropping NA: ' + str(len(data)))\n",
    "    \n",
    "    # remove comments from bots\n",
    "    data = data[~data['author'].str.lower().isin([x.lower() for x in botlist])]\n",
    "    # print('-- Rows after dropping from botlist: ' + str(len(data)))\n",
    "    \n",
    "    # remove more bots/moderators\n",
    "    data = data[~data.author.str.contains(pat='bot|moderator|b0t', case=False, na=False)].reset_index(drop=True)\n",
    "    # print('-- Rows after dropping more bots/moderators: ' + str(len(data)))\n",
    "    \n",
    "    # drop deleted authors\n",
    "    data = data.drop(data[data['author'] == '[deleted]'].index)\n",
    "    # print('-- Rows after dropping deleted authors: ' + str(len(data)))\n",
    "       \n",
    "    # remove duplicates based on body\n",
    "    data = data.drop_duplicates(subset=['body'], keep=False)\n",
    "    # print('-- Rows after dropping duplicates: ' + str(len(data)))\n",
    "\n",
    "    # reset index\n",
    "    data = data.reset_index(drop=True)   \n",
    "    \n",
    "    # print time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocessing - body, tokenize, remove short\n",
    "def tokenize(data):\n",
    "\n",
    "    # stop time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # remove line breaks from body\n",
    "    data['body'] = data['body'].str.replace('\\n',' ')\n",
    "\n",
    "    # remove links\n",
    "    pattern = r'http\\S+'\n",
    "    data['body'] = data['body'].str.replace(pat=pattern,repl=' ')\n",
    "\n",
    "    # lowercase, remove numbers and punctuation, tokenize\n",
    "    data['body'] = data.apply(lambda row: gensim.utils.simple_preprocess(row.body, deacc=False, min_len=1, max_len=15), axis=1)\n",
    "    data = data.rename(columns = {'body': 'body_tokens'})\n",
    "    \n",
    "    # remove comments with fewer than 5 tokens\n",
    "    data = data[data['body_tokens'].map(len) >= 5]\n",
    "    print('-- Rows after dropping short comments: ' + str(len(data)))\n",
    "\n",
    "    # reset index\n",
    "    data = data.reset_index(drop=True) \n",
    "\n",
    "    # create new column\n",
    "    data['no_tokens'] = data['body_tokens'].str.len()\n",
    "\n",
    "    # print time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing chunk 0\n",
      "-- Rows after dropping short comments: 784690\n",
      "--- Finished after 253.14268684387207 seconds ---\n",
      "--- Processing chunk 1\n",
      "-- Rows after dropping short comments: 785628\n",
      "--- Finished after 196.0009548664093 seconds ---\n",
      "--- Processing chunk 2\n",
      "-- Rows after dropping short comments: 785053\n",
      "--- Finished after 190.57830786705017 seconds ---\n",
      "--- Processing chunk 3\n",
      "-- Rows after dropping short comments: 785125\n",
      "--- Finished after 194.95415878295898 seconds ---\n",
      "--- Processing chunk 4\n",
      "-- Rows after dropping short comments: 785638\n",
      "--- Finished after 198.19109988212585 seconds ---\n",
      "--- Processing chunk 5\n",
      "-- Rows after dropping short comments: 787308\n",
      "--- Finished after 200.8927869796753 seconds ---\n",
      "--- Processing chunk 6\n",
      "-- Rows after dropping short comments: 787261\n",
      "--- Finished after 202.44296407699585 seconds ---\n",
      "--- Processing chunk 7\n",
      "-- Rows after dropping short comments: 788029\n",
      "--- Finished after 255.3637399673462 seconds ---\n",
      "--- Processing chunk 8\n",
      "-- Rows after dropping short comments: 780620\n",
      "--- Finished after 254.76312899589539 seconds ---\n",
      "--- Processing chunk 9\n",
      "-- Rows after dropping short comments: 779310\n",
      "--- Finished after 243.01187109947205 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Read chunked data, preprocess and save as compressed files\n",
    "os.chdir('directory')\n",
    "chunksize = 10**6\n",
    "i = 0\n",
    "for chunk in pd.read_csv('askreddit.gz',\n",
    "                         sep='|',\n",
    "                         compression='gzip',\n",
    "                         index_col=0,\n",
    "                         chunksize=chunksize,\n",
    "                         usecols=[0,1,2],\n",
    "                         lineterminator='\\n'):\n",
    "    \n",
    "    # stop time\n",
    "    start_time = time.time()\n",
    "    print('--- Processing chunk ' + str(i))\n",
    "    \n",
    "    # preprocess\n",
    "    chunk = tokenize(drop(chunk,botlist))\n",
    "    \n",
    "    # save as compressed file\n",
    "    os.chdir('/Users/jonas/Documents/SEDS/CSS/Project/Data/Comments/WorldNews')\n",
    "    chunk.to_csv('worldnews_pp_'+str(i)+'.gz',\n",
    "                 sep = '|',\n",
    "                 compression='gzip')\n",
    "    \n",
    "    print(\"--- Finished after %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "Compressed, preprocessed files are streamed into word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Stream compressed files to avoid RAM crash\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname,limit=None):\n",
    "        self.dirname = dirname\n",
    "        self.limit = limit\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Count number of tokens\n",
    "        no = 0 \n",
    "        # iterate through the file directory\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # for each compressed file open it\n",
    "            with gensim.utils.smart_open(os.path.join(self.dirname, fname)) as fin:             \n",
    "                for line in itertools.islice(fin, self.limit):\n",
    "                    try:\n",
    "                        tokens = ast.literal_eval(gensim.utils.to_unicode(line).split(\"|\")[1])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    no += 1\n",
    "                    yield tokens\n",
    "        # Print number of tokens\n",
    "        print(str(no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13251251\n",
      "13251251\n",
      "13251251\n",
      "13251251\n",
      "13251251\n",
      "13251251\n"
     ]
    }
   ],
   "source": [
    "# Read files and run word2vec \n",
    "assert gensim.models.word2vec.FAST_VERSION > -1\n",
    "path = 'directory_of_compressed_files'\n",
    "sentences = MySentences(path) # a memory-friendly iterator\n",
    "\n",
    "# Set parameters. Details here: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model = gensim.models.word2vec.Word2Vec(sentences,sg=1, \n",
    "                                        size=300, \n",
    "                                        window=5, \n",
    "                                        min_count=10, \n",
    "                                        workers=10, \n",
    "                                        hs=0, \n",
    "                                        negative=8,\n",
    "                                        iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5738304853439331),\n",
       " ('angmar', 0.4578200578689575),\n",
       " ('boleyn', 0.44060570001602173),\n",
       " ('nosmo', 0.4355197250843048),\n",
       " ('latifa', 0.4351215660572052),\n",
       " ('kings', 0.4347761869430542),\n",
       " ('latifah', 0.4287641644477844),\n",
       " ('arthurs', 0.426189124584198),\n",
       " ('soopers', 0.40994611382484436),\n",
       " ('nefertiti', 0.40902742743492126)]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classic test\n",
    "model.wv.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('/Users/jonas/Documents/SEDS/CSS/Project/Data/Models/askreddit500.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Save model vocabulary list and vectors seperately for further analysis in R\n",
    "def save_emb(model,name,directory):\n",
    "    syn0_object=model.wv.syn0\n",
    "\n",
    "    ##output vector space##\n",
    "    np.savetxt(directory+'syn0_ngf_'+name+'.txt',\n",
    "             syn0_object, delimiter=\" \")\n",
    "\n",
    "    #output vocab list#\n",
    "    vocab_list = model.wv.index2word\n",
    "    for i in range(0,len(vocab_list)):\n",
    "        if vocab_list[i] == '':\n",
    "            vocab_list[i] = \"thisisanemptytoken\"+str(i)\n",
    "\n",
    "    with open(directory+'vocab_list_ngf_'+name+'.txt','wb') as outfile:\n",
    "        for i in range(0,len(vocab_list)):\n",
    "            outfile.write(vocab_list[i].encode('utf8')+\"\\n\".encode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Runs several evaluation tests: \n",
    "- Google Semantic-Syntactic: http://download.tensorflow.org/data/questions-words.txt (split)\n",
    "- SimVerb-3005: https://github.com/JoonyoungYi/datasets/tree/master/simverb3500\n",
    "- MEN: https://staff.fnwi.uva.nl/e.bruni/MEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askreddit500 performance:\n",
      "\n",
      "Google semantic: 0.634\n",
      "Google syntactic: 0.6803\n",
      "Simverb Pearson: 0.3767\n",
      "MEN Pearson: 0.7513\n",
      "\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# directory to models/ and evaluation/ which includes tests\n",
    "os.chdir('directory')\n",
    "\n",
    "# models to be compared\n",
    "models = ['askreddit']\n",
    "\n",
    "# load each model and print results\n",
    "for model in models:\n",
    "    mod = gensim.models.word2vec.Word2Vec.load('Models/'+model+'.model')\n",
    "\n",
    "    print(model+' performance:\\n')\n",
    "    \n",
    "    sem = mod.wv.accuracy('Evaluation/semantic.txt')\n",
    "    print('Google semantic: '+ str(round(test_res(sem), 4)))\n",
    "\n",
    "    syn = mod.wv.accuracy('Evaluation/syntactic.txt')\n",
    "    print('Google syntactic: '+ str(round(test_res(syn), 4)))\n",
    "    \n",
    "    simverb = mod.wv.evaluate_word_pairs('Evaluation/simverb-3500.txt')[0][0]\n",
    "    print('Simverb Pearson: '+ str(round(simverb, 4)))\n",
    "\n",
    "    men = mod.wv.evaluate_word_pairs('Evaluation/men.txt')[0][0]\n",
    "    print('MEN Pearson: '+ str(round(men, 4)))\n",
    "\n",
    "    print('\\n------------\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
